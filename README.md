# ä¸­è‹±æœºå™¨ç¿»è¯‘é¡¹ç›® (ZHâ†’EN Machine Translation)

æœ¬é¡¹ç›®å®ç°äº†åŸºäºæ·±åº¦å­¦ä¹ çš„ä¸­æ–‡åˆ°è‹±æ–‡æœºå™¨ç¿»è¯‘ç³»ç»Ÿï¼ŒåŒ…å«ä¸¤ç§æ¨¡å‹æ¶æ„ï¼š**Transformer** å’Œ **RNN (Seq2Seq with Attention)**ã€‚

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
.
â”œâ”€â”€ config.yaml               # å…¨å±€é…ç½®æ–‡ä»¶ï¼ˆæ¨¡å‹/è®­ç»ƒ/æ•°æ®è·¯å¾„ï¼‰
â”œâ”€â”€ preprocess.py             # æ•°æ®é¢„å¤„ç†è„šæœ¬
â”œâ”€â”€ train.py                  # è®­ç»ƒè„šæœ¬ï¼ˆæ”¯æŒ Transformer å’Œ RNNï¼‰
â”œâ”€â”€ evaluate.py               # è¯„ä¼°è„šæœ¬ï¼ˆè®¡ç®— BLEU åˆ†æ•°ï¼‰
â”œâ”€â”€ check_translations.py     # ç¿»è¯‘ç»“æœæ ¼å¼éªŒè¯
â”œâ”€â”€ tokenizer.py              # åˆ†è¯å™¨åŸºç±»å’Œå®ç°
â”œâ”€â”€ utils.py                  # å·¥å…·å‡½æ•°ï¼ˆæ•°æ®é›†ã€ç¿»è¯‘å‡½æ•°ç­‰ï¼‰
â”œâ”€â”€ README.md                 # æœ¬æ–‡ä»¶
â”œâ”€â”€ README_en.md              # è‹±æ–‡è¯´æ˜æ–‡æ¡£
â”‚
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ transformer.py        # Transformer æ¨¡å‹å®ç°
â”‚   â””â”€â”€ rnn.py                # RNN Seq2Seq æ¨¡å‹å®ç°
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train_100k.jsonl      # å¤§è®­ç»ƒé›†ï¼ˆ100k æ ·æœ¬ï¼‰
â”‚   â”œâ”€â”€ train_10k.jsonl       # å°è®­ç»ƒé›†ï¼ˆ10k æ ·æœ¬ï¼‰
â”‚   â”œâ”€â”€ valid.jsonl           # éªŒè¯é›†ï¼ˆ500 æ ·æœ¬ï¼‰
â”‚   â”œâ”€â”€ test.jsonl            # æµ‹è¯•é›†ï¼ˆ200 æ ·æœ¬ï¼‰
â”‚   â””â”€â”€ processed/            # é¢„å¤„ç†åçš„æ•°æ®ç›®å½•
â”‚
â””â”€â”€ runs/                     # æ¨¡å‹æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å®‰è£…ä¾èµ–
pip install torch pyyaml jieba sacrebleu tqdm
```

**ç¯å¢ƒè¦æ±‚ï¼š**
- Python 3.10+
- PyTorch 2.0+
- CUDAï¼ˆå¯é€‰ï¼Œç”¨äº GPU åŠ é€Ÿï¼‰

### 2. æ•°æ®å‡†å¤‡

å°†æ•°æ®æ–‡ä»¶æ”¾å…¥ `data/` ç›®å½•ä¸‹ï¼Œç„¶åè¿è¡Œé¢„å¤„ç†ï¼š

```bash
python preprocess.py -c config.yaml
```

è¿™å°†ç”Ÿæˆï¼š
- `data/processed/train.jsonl` - é¢„å¤„ç†åçš„è®­ç»ƒæ•°æ®
- `data/processed/val.jsonl` - é¢„å¤„ç†åçš„éªŒè¯æ•°æ®
- `data/processed/test.jsonl` - é¢„å¤„ç†åçš„æµ‹è¯•æ•°æ®
- `data/processed/src_vocab.pkl` - æºè¯­è¨€ï¼ˆä¸­æ–‡ï¼‰è¯è¡¨
- `data/processed/tgt_vocab.pkl` - ç›®æ ‡è¯­è¨€ï¼ˆè‹±æ–‡ï¼‰è¯è¡¨

### 3. æ¨¡å‹è®­ç»ƒ

#### è®­ç»ƒ Transformer æ¨¡å‹

```bash
# ç¡®ä¿ config.yaml ä¸­ model_type: transformer
python train.py -c config.yaml
```

#### è®­ç»ƒ RNN æ¨¡å‹

ä¿®æ”¹ `config.yaml`ï¼š
```yaml
model_type: rnn
```

ç„¶åè¿è¡Œï¼š
```bash
python train.py -c config.yaml
```

### 4. æ¨¡å‹è¯„ä¼°

```bash
python evaluate.py -c config.yaml --ckpt runs/model_epoch_10.pt --save_path translations.json
```

### 5. éªŒè¯è¾“å‡ºæ ¼å¼

```bash
python check_translations.py translations.json
```

---

## âš™ï¸ é…ç½®è¯´æ˜

### config.yaml å®Œæ•´é…ç½®

```yaml
# ------------- åˆ†è¯å™¨ -----------------
tokenizer: tokenizer.JiebaEnTokenizer

# ------------- æ¨¡å‹é€‰æ‹© ----------------
# å¯é€‰: "transformer" æˆ– "rnn"
model_type: transformer

# ------------- æ¨¡å‹ç»“æ„ ----------------
model:
  # === é€šç”¨å‚æ•° ===
  enc_layers: 4          # Encoder å±‚æ•°
  dec_layers: 4          # Decoder å±‚æ•°
  emb_size: 256          # è¯å‘é‡ç»´åº¦
  dropout: 0.1           # Dropout æ¦‚ç‡
  
  # === Transformer ä¸“ç”¨å‚æ•° ===
  nhead: 8               # Multi-Head Attention å¤´æ•°
  ffn_dim: 1024          # Feed-Forward éšå±‚ç»´åº¦
  
  # === RNN ä¸“ç”¨å‚æ•° ===
  rnn_type: gru          # RNN ç±»å‹: "gru" æˆ– "lstm"
  hidden_size: 512       # RNN éšè—å±‚ç»´åº¦
  attention_method: dot  # æ³¨æ„åŠ›æ–¹æ³•: "dot", "multiplicative", "additive"

# ------------- è®­ç»ƒè¶…å‚ ----------------
train:
  batch_size: 64
  epochs: 10
  lr: 0.0003
  weight_decay: 0.0001
  lr_step: 8
  lr_gamma: 0.5
  save_dir: runs
  num_workers: 0

# ------------- æ•°æ®è·¯å¾„ ----------------
data:
  raw_train: data/train_10k.jsonl
  raw_val: data/valid.jsonl
  raw_test: data/test.jsonl
  processed_dir: data/processed
  train_processed: data/processed/train.jsonl
  val_processed: data/processed/val.jsonl
  test_processed: data/processed/test.jsonl
  src_vocab: data/processed/src_vocab.pkl
  tgt_vocab: data/processed/tgt_vocab.pkl
  min_freq: 1

# ------------- å…¶ä½™ --------------------
seed: 3407
```

---

## ğŸ—ï¸ æ¨¡å‹æ¶æ„

### 1. Transformer æ¨¡å‹

åŸºäº "Attention Is All You Need" è®ºæ–‡å®ç°ï¼ŒåŒ…å«ï¼š

| ç»„ä»¶ | è¯´æ˜ |
|------|------|
| **PositionalEncoding** | æ­£å¼¦/ä½™å¼¦ä½ç½®ç¼–ç ï¼Œæ³¨å…¥åºåˆ—ä½ç½®ä¿¡æ¯ |
| **MultiHeadAttention** | å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ”¯æŒ Q/K/V æŠ•å½±å’Œç¼©æ”¾ç‚¹ç§¯ |
| **EncoderLayer** | è‡ªæ³¨æ„åŠ› + FFN + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ– |
| **DecoderLayer** | æ©ç è‡ªæ³¨æ„åŠ› + äº¤å‰æ³¨æ„åŠ› + FFN |
| **Encoder** | N å±‚ EncoderLayer å †å  |
| **Decoder** | N å±‚ DecoderLayer å †å  |

**æ¨¡å‹æµç¨‹ï¼š**
```
æºåºåˆ— â†’ Embedding â†’ PositionalEncoding â†’ Encoder Layers â†’ Memory
ç›®æ ‡åºåˆ— â†’ Embedding â†’ PositionalEncoding â†’ Decoder Layers (with Memory) â†’ Linear â†’ Softmax
```

### 2. RNN Seq2Seq æ¨¡å‹

åŸºäº GRU/LSTM çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼ŒåŒ…å«ä¸‰ç§æ³¨æ„åŠ›æœºåˆ¶ï¼š

| æ³¨æ„åŠ›æ–¹æ³• | è®¡ç®—å…¬å¼ |
|-----------|---------|
| **Dot (ç‚¹ç§¯)** | $score = h_t^T \cdot h_s$ |
| **Multiplicative (ä¹˜æ³•)** | $score = h_t^T W h_s$ |
| **Additive (åŠ æ³•)** | $score = v^T \tanh(W_1 h_t + W_2 h_s)$ |

**æ¨¡å‹ç»„ä»¶ï¼š**

| ç»„ä»¶ | è¯´æ˜ |
|------|------|
| **RNNEncoder** | 2 å±‚å•å‘ GRU/LSTMï¼Œå°†æºåºåˆ—ç¼–ç ä¸ºéšè—çŠ¶æ€ |
| **Attention** | æ”¯æŒç‚¹ç§¯ã€ä¹˜æ³•ã€åŠ æ³•ä¸‰ç§å¯¹é½å‡½æ•° |
| **RNNDecoder** | å¸¦æ³¨æ„åŠ›æœºåˆ¶çš„è§£ç å™¨ï¼Œä½¿ç”¨ Teacher Forcing è®­ç»ƒ |

**æ¨¡å‹æµç¨‹ï¼š**
```
æºåºåˆ— â†’ Embedding â†’ RNN Encoder â†’ (encoder_outputs, hidden)
                                         â†“
ç›®æ ‡åºåˆ— â†’ Embedding â†’ RNN Decoder (with Attention) â†’ Linear â†’ Softmax
```

---

## ğŸ“Š ä½¿ç”¨ç¤ºä¾‹

### åˆ‡æ¢æ¨¡å‹ç±»å‹

**ä½¿ç”¨ Transformerï¼š**
```yaml
model_type: transformer
model:
  enc_layers: 4
  dec_layers: 4
  emb_size: 256
  nhead: 8
  ffn_dim: 1024
```

**ä½¿ç”¨ GRU + ç‚¹ç§¯æ³¨æ„åŠ›ï¼š**
```yaml
model_type: rnn
model:
  enc_layers: 2
  dec_layers: 2
  emb_size: 256
  hidden_size: 512
  rnn_type: gru
  attention_method: dot
```

**ä½¿ç”¨ LSTM + åŠ æ³•æ³¨æ„åŠ›ï¼š**
```yaml
model_type: rnn
model:
  enc_layers: 2
  dec_layers: 2
  emb_size: 256
  hidden_size: 512
  rnn_type: lstm
  attention_method: additive
```

### è®­ç»ƒå‘½ä»¤

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®
python train.py -c config.yaml

# ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
python train.py -c config_rnn.yaml
```

### è¯„ä¼°å‘½ä»¤

```bash
# è¯„ä¼°å¹¶ä¿å­˜ç¿»è¯‘ç»“æœ
python evaluate.py -c config.yaml --ckpt runs/model_epoch_10.pt --save_path translations.json

# æŸ¥çœ‹ BLEU åˆ†æ•°å’Œç¿»è¯‘æ ·ä¾‹
```

---

## ğŸ“ è¾“å‡ºæ ¼å¼

è¯„ä¼°è„šæœ¬è¾“å‡ºçš„ `translations.json` æ ¼å¼ï¼š

```json
[
  {
    "src": "ä»Šå¤©å¤©æ°”å¾ˆå¥½",
    "ref": "It is a fine day today",
    "hyp": "The weather is great today",
    "hyp_id": "sha256_hash..."
  },
  ...
]
```

---

## ğŸ”§ å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•é€‰æ‹© Transformer è¿˜æ˜¯ RNNï¼Ÿ

- **Transformer**ï¼šé€‚åˆè¾ƒé•¿åºåˆ—ï¼Œå¹¶è¡Œè®¡ç®—æ•ˆç‡é«˜ï¼Œæ•ˆæœé€šå¸¸æ›´å¥½
- **RNN**ï¼šå‚æ•°é‡è¾ƒå°‘ï¼Œé€‚åˆèµ„æºå—é™ç¯å¢ƒï¼Œæ›´å®¹æ˜“ç†è§£å’Œè°ƒè¯•

### Q2: å¦‚ä½•è°ƒæ•´æ¨¡å‹å¤§å°ï¼Ÿ

- å‡å° `emb_size` å’Œ `hidden_size` å¯ä»¥å‡å°‘å‚æ•°é‡
- å‡å°‘ `enc_layers` å’Œ `dec_layers` å¯ä»¥åŠ å¿«è®­ç»ƒé€Ÿåº¦
- å¯¹äº Transformerï¼Œå‡å°‘ `nhead` å’Œ `ffn_dim` ä¹Ÿå¯ä»¥å‡å°æ¨¡å‹

### Q3: è®­ç»ƒæ—¶ OOMï¼ˆæ˜¾å­˜ä¸è¶³ï¼‰æ€ä¹ˆåŠï¼Ÿ

1. å‡å° `batch_size`
2. å‡å°æ¨¡å‹è§„æ¨¡ï¼ˆå±‚æ•°ã€ç»´åº¦ï¼‰
3. ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
4. ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ

### Q4: å¦‚ä½•æé«˜ BLEU åˆ†æ•°ï¼Ÿ

1. ä½¿ç”¨æ›´å¤§çš„è®­ç»ƒé›†ï¼ˆ100k è€Œé 10kï¼‰
2. å¢åŠ è®­ç»ƒè½®æ•°
3. è°ƒæ•´å­¦ä¹ ç‡å’Œæ­£åˆ™åŒ–å‚æ•°
4. å°è¯•ä¸åŒçš„æ¨¡å‹æ¶æ„å’Œæ³¨æ„åŠ›æœºåˆ¶

---

## ğŸ“š å‚è€ƒèµ„æ–™

1. Vaswani, A., et al. "Attention Is All You Need." NeurIPS 2017.
2. Bahdanau, D., et al. "Neural Machine Translation by Jointly Learning to Align and Translate." ICLR 2015.
3. Luong, M., et al. "Effective Approaches to Attention-based Neural Machine Translation." EMNLP 2015.

---

## ğŸ“„ æäº¤æ¸…å•

æäº¤å‰è¯·ç¡®ä¿åŒ…å«ä»¥ä¸‹æ–‡ä»¶ï¼š

- [ ] `model/transformer.py` - å®Œæ•´çš„ Transformer å®ç°
- [ ] `model/rnn.py` - å®Œæ•´çš„ RNN Seq2Seq å®ç°
- [ ] `runs/best_model.pt` - æœ€ä½³æ¨¡å‹æ£€æŸ¥ç‚¹
- [ ] `translations.json` - ç¿»è¯‘ç»“æœæ–‡ä»¶
- [ ] é¡¹ç›®æŠ¥å‘Š (PDF)

éªŒè¯è¾“å‡ºæ ¼å¼ï¼š
```bash
python check_translations.py translations.json
```

---

## ğŸ‘¨â€ğŸ’» ä½œè€…

2025 ç§‹å­£å­¦æœŸ - äººå·¥ç¥ç»ç½‘ç»œæœŸæœ«é¡¹ç›®
